# data-515-mediquery

# MediQuery: Multimodal Health Assistant

## Project Title
**MediQuery: Multimodal Health Assistant**

## Team Members
- Stuti Gaonkar
- Sowmya Dhadheech
- Catherine Wu
- Jolene Pern

## Project Type
**Research Project**

## Questions of Interest

1. **How can multimodal AI improve healthcare accessibility?**
   - Can we reduce language barriers in healthcare by enabling patients to communicate through images and voice?
   - What impact does multimodal input have on diagnostic accuracy compared to text-only systems?

2. **What are the optimal fusion strategies for medical multimodal data?**
   - How should visual, textual, and audio signals be weighted and combined for medical assessment?
   - What architectures best capture cross-modal relationships in health data?

## Project Goal & Output

We will produce a **multimodal health assistant web application** that allows patients to describe symptoms using:
- **Text** - Natural language symptom descriptions
- **Images** - Photos of skin conditions or injuries  
- **Voice** - Audio recordings for cough analysis

**Deliverables:**
- Multimodal health assistant web application
- Research findings on multimodal fusion for healthcare

## Data Sources

### 1. NIH MedPix Medical Image Database
- **Content:** 10,000+ medical images with clinical diagnoses and annotations
- **URL:** https://medpix.nlm.nih.gov/
- **Use:** Training vision models for medical image analysis

### 2. CDC Symptom & Disease Dataset
- **Content:** Text-based symptom descriptions paired with disease outcomes
- **Source:** Centers for Disease Control and Prevention (CDC)
- **Use:** NLP for symptom understanding and disease mapping

### 3. Respiratory Sound Database (Planned)
- **Content:** Audio recordings of coughs, wheezes, and breathing patterns
- **Use:** Audio analysis for respiratory condition assessment

---

**Note:** This is a research project for educational purposes and is NOT a replacement for professional medical advice.
